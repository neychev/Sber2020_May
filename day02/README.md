Comparing Random Forest and Gradient boosting:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neychev/Sber2020_May/blob/master/day02/day02_ComparingRandomForestAndGradientBoosting.ipynb)

Practice feature importances:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neychev/Sber2020_May/blob/master/day02/day02_feature_importance_exercises.ipynb)

Intro to DL:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neychev/Sber2020_May/blob/master/day02/day02_intro_to_pytorch.ipynb)



__Further readings__:


__On Boosting and feature importances:__
__Further readings__:
* [en] Bias-variance tradeoff in more general case: A Unified Bias-Variance Decomposition and its Applications https://homes.cs.washington.edu/~pedrod/papers/mlc00a.pdf
* [ru] Evgeny Sokolov notes about bias-variance decomposition and Random Forest: https://github.com/esokolov/ml-course-hse/blob/master/2018-fall/lecture-notes/lecture08-ensembles.pdf
* [ru] Great blog post about stacking, blending and their modifications: https://dyakonov.org/2017/03/10/cтекинг-stacking-и-блендинг-blending/
* [ru] Alexander Guschin Bachelor's thesis about stacking: http://www.machinelearning.ru/wiki/images/5/56/Guschin2015Stacking.pdf
* [ru] Great ODS blogpost about gradient boosting: https://habr.com/ru/company/ods/blog/327250/
* [en] Same post as 3 but in English: https://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic10_boosting/topic10_gradient_boosting.ipynb
* [en] Great interactive blogpost by Alex Rogozhnikov: http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html
* [en] And great gradient boosted trees playground by Alex Rogozhnikov: http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html
* [en] Shap values repo and explanation: https://github.com/slundberg/shap
* [en] Kaggle tutorial on feature importances: https://www.kaggle.com/learn/machine-learning-explainability


__On Deep Learning basics:__
* [en] Notes on vector and matrix derivatives: http://cs231n.stanford.edu/vecDerivs.pdf
* [en] Stanford notes on backpropagation: http://cs231n.github.io/optimization-2/
* [en] Stanford notes on different activation functions (and just intuition): http://cs231n.github.io/neural-networks-1/
* [en] Great post on Medium by Andrej Karpathy: https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b
* [en] CS231n notes on data preparation (batch normalization over there): http://cs231n.github.io/neural-networks-2/
* [en] CS231n notes on gradient methods: http://cs231n.github.io/neural-networks-3/
* [en] Original paper introducing Batch Normalization: https://arxiv.org/pdf/1502.03167.pdf
* [en] What Every Computer Scientist Should Know About Floating-Point Arithmetic: https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html